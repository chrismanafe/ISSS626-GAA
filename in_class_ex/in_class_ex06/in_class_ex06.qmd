---
title: "In-class Exercise 6: Emerging Hot Spot Analysis"
author: "Christover Manafe"
date: "2024-09-30"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
format: 
  html:
    code-fold: false
    code-summary: "code chunk"
    number-sections: true
    number-depth: 4
---

# Overview

Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:

-   Building a space-time cube,
-   Calculating Getis-Ord local Gi\* statistic for each bin by using an FDR correction,
-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test,
-   Categorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.

# The Data

We will use following geospatial datasets in this exercise:

| Dataset       | Description                                 | Format         |
|-------------------|--------------------------------|---------------------|
| *Hunan*       | Hunan county boundary layer geospatial data | ESRI shapefile |
| *Hunan_GDPPC* | Contains Hunan’s historical GDPPC data.     | CSV file       |

: {tbl-colwidths="\[15,65,20\]"}

# Installing and launching the R packages

::: panel-tabset
## Packages

We will use following packages in this exercise:

| Package                                                   | Description                                                                                                                                                                                                          |
|---------------------------|---------------------------------------------|
| [**sf**](https://r-spatial.github.io/sf/)                 | Provides functions to manage, processing, and manipulate **Simple Features**, a formal geospatial data standard that specifies a storage and access model of spatial geometries such as points, lines, and polygons. |
| [sfdep](https://cran.r-project.org/web/packages/sfdep/)   | Provides collection of functions to create spatial weights matrix objects from polygon 'contiguities', from point patterns by distance and tessellations.                                                            |
| [**tidyverse**](https://www.tidyverse.org/)               | Provides collection of functions for performing data science task such as importing, tidying, wrangling data and visualising data.                                                                                   |
| [**tmap**](https://cran.r-project.org/web/packages/tmap/) | Provides functions for plotting cartographic quality static point patterns maps or interactive maps by using [leaflet](https://leafletjs.com/) API                                                                   |

: {tbl-colwidths="\[15,85\]"}

## Code

To install and launch the four R packages.

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse)
```
:::

# Data Import and Preparation

## Loading the data

In this section, we will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.

::: panel-tabset
## Import geospatial data

We use [*st_read()*](https://r-spatial.github.io/sf/reference/st_read.html) of **sf** package to import Hunan shapefile into R. The imported shapefile will be **simple features** Object of **sf**.

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

```{r}
#| echo: false
glimpse(hunan)
```

## Import aspatial data

Then we will import *Hunan_2012.csv* into R by using *read_csv()* of **readr** package. The output is R dataframe class.

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

```{r}
#| echo: false
glimpse(GDPPC)
```

## Creating a Time Series Cube

In the code chunk below, [`spacetime()`](https://sfdep.josiahparry.com/reference/spacetime.html) of **sfdep** is used to create an spatio-temporal cube.

```{r}
GDPPC_st <- spacetime(GDPPC, hunan,
                      .loc_col = "County",
                      .time_col = "Year")
```

Next, we will use `is_spacetime_cube()` of **sfdep** package to verify if `GDPPC_st` is indeed an space-time cube object.

```{r}
is_spacetime_cube(GDPPC_st)
```

> The result confirms that *GDPPC_st* object is indeed an time-space cube.
:::

# Hot spot and cold spot area analysis

## Computing local Gi\* statistics

Next, we will compute the local Gi\* statistics.

### Deriving the spatial weights

As with most spatial analyses, we first need to derive a spatial weight matrix before computing the local Gi\* statistics. The code chunk below demonstrates how to derive a spatial weight matrix using functions from the `sfdep` package, combined with the `tidyverse` approach.

```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

> -   `activate()` of dplyr package is used to activate the geometry context
> -   `mutate()` of dplyr package is used to create two new columns *nb* and *wt*.
> -   Then we will activate the data context again and copy over the nb and wt columns to each time-slice using `set_nbs()` and `set_wts()`
>     -   row order is very important so do not rearrange the observations after using `set_nbs()` or `set_wts()`.

### Compute Gi\*

We can manually calculate the local Gi\* for each location using the new columns. This is done by grouping the data by *Year* and applying the `local_gstar_perm()` function from the `sfdep` package. Afterward, we can use `unnest()` to unnest the *gi_star* column from the newly created *gi_stars* data frame.

```{r}
gi_stars <- GDPPC_nb %>% 
  group_by(Year) %>% 
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt)) %>% 
  tidyr::unnest(gi_star)
```

## Mann-Kendall Test

A **monotonic series** or function is one that only increases or decreases and never changes direction. As long as the function either stays flat or continues to increase (or decrease), it is considered monotonic.

-   **H₀ (Null Hypothesis):** There is no monotonic trend.

-   **H₁ (Alternative Hypothesis):** A monotonic trend is present.

**Interpretation:**

-   Reject the null hypothesis (H₀) if the p-value is smaller than the alpha level (i.e., 1 - confidence level).

-   **Tau (τ)** ranges between -1 and 1, where:

    -   **-1** represents a perfectly decreasing series.

    -   **1** represents a perfectly increasing series.

### Mann-Kendall Test on Gi

With these Gi\* measures we can then evaluate each location for a trend using the Mann-Kendal test. Let's use it on Changsha county.

```{r}
cbg <- gi_stars %>% 
  ungroup() %>% 
  filter(County == "Changsha") |> 
  select(County, Year, gi_star)
```

### Visualize Mann-Kendall Test Result

Next, we plot the result by using ggplot2 functions.

```{r}
ggplot(data = cbg, 
       aes(x = Year, 
           y = gi_star)) +
  geom_line() +
  theme_light()
```

We can also create an interactive plot by using `ggplotly()` of **plotly** package.

```{r}
p <- ggplot(data = cbg, 
       aes(x = Year, 
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

### Print Mann-Kendall test report

```{r}
cbg %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

> In the above result, **sl** is the p-value. With reference to the results, we will reject the null hypothesis and infer there's a slight upward trend.

### Mann-Kendall test data.frame

We can replicate this for each location by using `group_by()` of **dplyr** package.

```{r}
ehsa <- gi_stars %>%
  group_by(County) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
head(ehsa)
```

#### Arrange significant emerging hot/cold spots

We can also sort to show significant hot spots using following code chunk.

```{r}
emerging <- ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:10)
emerging
```

### Performing Emerging Hotspot Analysis

Lastly, we will perform EHSA analysis by using [`emerging_hotspot_analysis()`](https://sfdep.josiahparry.com/reference/emerging_hotspot_analysis.html) of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.

```{r}
ehsa <- emerging_hotspot_analysis(
  x = GDPPC_st, 
  .var = "GDPPC", 
  k = 1, 
  nsim = 99
)
```

### Visualising the distribution of EHSA classes

We'll visualise the distribution of EHSA classes using `ggplot2` functions.

```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

> The figure above shows that sporadic cold spots class has the high numbers of county.

### Visualising EHSA

In this section, we will visualise the geographic distribution EHSA classes. However, before we can do so, we need to join both *hunan* and *ehsa* together by using the code chunk below.

```{r}
hunan_ehsa <- hunan %>%
  left_join(ehsa,
            by = join_by(County == location))
```

Next, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.

```{r fig.height=12, fig.width=8}
ehsa_sig <- hunan_ehsa  %>%
  filter(p_value < 0.05)
tmap_mode("plot")
tm_shape(hunan_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```

# Reference

Kam, T. S. Emerging Hot Spot Analysis. *ISSS626 Geospatial Analytics and Applications.* <https://isss626-ay2024-25aug.netlify.app/in-class_ex/in-class_ex06/in-class_ex06>

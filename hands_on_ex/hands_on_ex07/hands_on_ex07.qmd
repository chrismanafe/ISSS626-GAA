---
title: "Hands-on Exercise 7: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method"
author: "Christover Manafe"
date: "2024-09-25"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
format: 
  html:
    code-fold: false
    code-summary: "code chunk"
    number-sections: true
    number-depth: 4
---

# Overview

**Geographically Weighted Regression (GWR)** is a spatial statistical technique that accounts for non-stationary variables (e.g., climate, demographic factors, physical environment characteristics) to model the local relationships between these independent variables and a dependent variable, or outcome of interest.

In this hands-on exercise, we will learn to build [hedonic pricing](https://www.investopedia.com/terms/h/hedonicpricing.asp) models using GWR methods. The dependent variable in this exercise is the resale prices of condominiums in 2015, while the independent variables are categorized as either structural or locational factors.

# The data

| **Dataset Name**                  | **Description**                                                                                                          | **Format**     |
|-------------------|----------------------------------|-------------------|
| Master Plan 2014 Subzone Boundary | Geospatial data representing the boundaries of different areas in Singapore, specifically at the planning subzone level. | ESRI Shapefile |
| `condo_resale_2015`               | Aspatial data containing records of condominium resale history in Singapore for the year 2015.                           | CSV            |

: {tbl-colwidths="\[25,50,25\]"}

# The packages

::: panel-tabset
## Packages

We will use following packages in this exercise:

| **Package**                                                                   | **Description**                                                                                                                                                                                                   |
|------------------------|-----------------------------------------------|
| [**sf**](https://r-spatial.github.io/sf/)                                     | Provides functions to manage, process, and manipulate **Simple Features**, a formal geospatial data standard that specifies a storage and access model of spatial geometries such as points, lines, and polygons. |
| [**spdep**](https://cran.r-project.org/web/packages/spdep/)                   | Provides a collection of functions to create spatial weights matrix objects from polygon 'contiguities', point patterns by distance, and tessellations.                                                           |
| [**tidyverse**](https://www.tidyverse.org/)                                   | A collection of R packages for data science tasks such as importing, tidying, wrangling, and visualizing data.                                                                                                    |
| [**tmap**](https://cran.r-project.org/web/packages/tmap/)                     | Provides functions for creating cartographic-quality static maps or interactive maps using the [leaflet](https://leafletjs.com/) API.                                                                             |
| [**gtsummary**](https://cran.r-project.org/web/packages/gtsummary/index.html) | Provides functions to create presentation-ready tables that summarize datasets, regression models, and more.                                                                                                      |
| [**ggpubr**](https://rpkgs.datanovia.com/ggpubr/)                             | A collection of 'ggplot2'-based functions to easily create and customize publication-ready plots.                                                                                                                 |
| [**corrplot**](https://cran.r-project.org/web/packages/corrplot/)             | Provides functions for visualizing correlation matrices.                                                                                                                                                          |
| [**olsrr**](https://olsrr.rsquaredacademy.com/)                               | A package for building OLS models and performing diagnostic tests.                                                                                                                                                |
| [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/index.html)     | A package for calibrating the geographically weighted family of models.                                                                                                                                           |

: {tbl-colwidths="\[15,85\]"}

## Code

To install and launch all R packages.

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```
:::

::: callout-note
## GWmodel

The **GWmodel** package offers a collection of localized spatial statistical methods, including Geographically Weighted (GW) summary statistics, GW principal components analysis, GW discriminant analysis, and various forms of GW regression. Some of these methods are available in both basic and robust (outlier-resistant) forms. Typically, the outputs or parameters of the GWmodel are mapped, providing a valuable exploratory tool that can often precede and guide more traditional or advanced statistical analyses.
:::

# Data Import and Preparation

::: panel-tabset
## Geospatial data

Here, we will import the **MP14_SUBZONE_WEB_PL** shapefile using the `st_read()` function from the **sf** package.

```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

Next, we will update the newly imported **mpsz** dataset with the correct EPSG code (i.e., 3414).

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

After transforming the projection metadata, we can verify the projection of the newly transformed **mpsz_svy21** by using the `st_crs()` function from the **sf** package.

```{r}
st_crs(mpsz_svy21)
```

Notice that the EPSG code is now indicated as 3414.

Next, we will reveal the extent of **mpsz_svy21** using the `st_bbox()` function from the **sf** package.

```{r}
st_bbox(mpsz_svy21)
```

## Aspatial data

To import aspatial data in CSV format, we will use the `read_csv()` function from the **readr** package to load **condo_resale_2015** into R as a tibble data frame called **condo_resale**.

```{r}
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")
glimpse(condo_resale)
```

Let’s display the summary statistics of the **condo_resale** tibble data frame using the `summary()` function.

```{r}
summary(condo_resale)
```
:::

## Aspatial data wrangling

```{r}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
```

> Notice that the `st_transform()` function from the **sf** package is used to convert the coordinates from WGS84 (i.e., CRS: 4326) to SVY21 (i.e., CRS: 3414).

Let’s list the contents of the **condo_resale.sf** object.

```{r}
head(condo_resale.sf)
```

> Notice that the output is in point feature data frame.

# Exploratory Data Analysis (EDA)

## EDA using statistical graphics

### Plot distribution

We can plot the distribution of **SELLING_PRICE** using appropriate Exploratory Data Analysis (EDA) techniques, as shown in the code chunk below.

```{r}
ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

::: callout-note
**Observations:**

-   The distribution is right-skewed.
-   This indicates that more condominium units were transacted at relatively lower prices.
-   Statistically, the skewed distribution can be normalized using a log transformation.
:::

### Normalise using Log Transformation

The code chunk below derives a new variable called **LOG_SELLING_PRICE** by applying a log transformation to the **SELLING_PRICE** variable. This is done using the `mutate()` function from the **dplyr** package.

```{r}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

### Plot of normalised selling price

```{r}
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

> Notice that the distribution is relatively less skewed after the log transformation.

### Multiple Histogram Plots: Distribution of Variables

In this section, we will create small multiple histograms (also known as a trellis plot) using the `ggarrange()` function from the **ggpubr** package.

The code chunk below generates 12 histograms, which are then organized into a 3-column by 4-row small multiple plot using `ggarrange()`.

```{r}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

### Drawing Statistical Point Map

Lastly, we will visualize the geospatial distribution of condominium resale prices in Singapore. The map will be created using the **tmap** package.

```{r}
tmap_mode("view")
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

> The `set.zoom.limits` argument of `tm_view()` sets the minimum and maximum zoom levels to 11 and 14, respectively.

# Hedonic Pricing Modelling in R

## Simple Linear Regression Method

### Build Simple Linear Regression model

We will build a simple linear regression model using **SELLING_PRICE** as the dependent variable and **AREA_SQM** as the independent variable.

We will use the `lm()` function from base R. The `lm()` function returns an object of class "lm" or, for multiple responses, of class `c("mlm", "lm")`.

The functions `summary()` and `anova()` can be used to obtain and print a summary and an analysis of variance table for the results. Additionally, generic accessor functions like `coefficients()`, `effects()`, `fitted.values()`, and `residuals()`can be used to extract various useful features from the object returned by `lm()`.

```{r}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, 
                data = condo_resale.sf)

summary(condo.slr)
```

::: callout-note
The output report reveals that **SELLING_PRICE** can be explained by the following formula:

$$ y = -258,121.1 + 14,719 \times x_1 $$

where ( $x_1$ ) represents **AREA_SQM**.

-   The R-squared value of `0.4518` indicates that this simple regression model explains approximately 45% of the variation in resale prices.

-   Since the p-value is significantly smaller than `0.0001`, we reject the null hypothesis that the mean is a good estimator of **SELLING_PRICE**. This suggests that the simple linear regression model is a better estimator for **SELLING_PRICE**.

-   The **Coefficients** section of the report shows that the p-values for both the intercept and **AREA_SQM** estimates are smaller than `0.001`. Given this, we can reject the null hypothesis that B0 and B1 are equal to 0. As a result, we infer that B0 and B1 are reliable parameter estimates for this model.
:::

### Visualise best fit curve

Next, we will visualize the best fit curve on a scatterplot by using `lm()` as the method function in **ggplot**'s geometry.

```{r}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

> Figure above reveals that there are a few statistical outliers with relatively high selling prices.

## Multiple Linear Regression Method

### Visualising the relationships of the independent variables

Before building a multiple regression model, it is essential to ensure that the independent variables are not highly correlated with each other. Using highly correlated variables in a regression model can compromise its quality, a statistical phenomenon known as **multicollinearity**.

A correlation matrix is commonly used to visualize the relationships between independent variables. Besides the `pairs()`function in base R, there are many packages that support the display of a correlation matrix. In this section, we will use the **corrplot** package.

Reordering the matrix is crucial for uncovering hidden structures and patterns within the data. The **corrplot** package provides four methods for reordering the matrix (via the `order` parameter): "AOE", "FPC", "hclust", and "alphabet". In the code chunk below, the **AOE** order is used, which arranges the variables based on the angular order of eigenvectors, as suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

```{r fig.height=8, fig.width=10}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

> -   From the scatterplot matrix, it is clear that **Freehold** is highly correlated with **LEASE_99YEAR**.
> -   Given this, it is more prudent to include only one of these variables in the subsequent model building.
> -   As a result, **LEASE_99YEAR** is excluded from the subsequent model.

## Building a hedonic pricing model using multiple linear regression method

### Calibrate the multiple linear regression model

We will use the `lm()` function to calibrate the multiple linear regression model, incorporating the selected independent variables to predict the dependent variable.

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)
summary(condo.mlr)
```

::: {.callout-note appearance="simple"}
### Observations:

1.  **Significant Variables:**
    -   **AREA_SQM** has a positive and significant effect on **SELLING_PRICE** (Estimate = 12,708.32, p \< 2e-16).
    -   **AGE** negatively impacts **SELLING_PRICE** (Estimate = -24,440.82, p \< 2e-16).
    -   Proximity to **CBD**, **Childcare**, and **MRT** have significant negative effects, while proximity to **Parks**, **Primary Schools**, and **Bus Stops** positively affect **SELLING_PRICE**.
    -   **FREEHOLD** status significantly increases the selling price (Estimate = 359,913.01, p \< 4.38e-13).
2.  **Insignificant Variables:**
    -   Some variables, such as **PROX_HAWKER_MARKET**, **PROX_TOP_PRIMARY_SCH**, and **PROX_SUPERMARKET**, do not show significant influence on **SELLING_PRICE** (p-values \> 0.05).
3.  **Model Fit:**
    -   The **Multiple R-squared** value is 0.6518, indicating that approximately 65% of the variability in **SELLING_PRICE** is explained by this model.

    -   The **Adjusted R-squared** is 0.6474, slightly lower, accounting for the number of predictors.

    -   The **F-statistic** of 147.4 and p-value \< 2.2e-16 show that the model overall is highly significant.
:::

### Calibrate the revised model

With reference to the report above, it is clear that not all the independent variables are statistically significant. We will revise the model by removing those variables that are not statistically significant.

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)
ols_regress(condo.mlr1)
```

> In this step, our **condo.mlr1** object will contain the coefficients, residuals, effects, and fitted values. We will later extract the residuals as a dataframe to examine them closely.

### Preparing Publication Quality Table: gtsummary method

The [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/index.html) package provides an elegant and flexible way to create publication-ready summary tables in R.

In the code chunk below, [`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) is used to create a well formatted regression report.

```{r}
tbl_regression(condo.mlr1, intercept = TRUE)
```

Using the **gtsummary** package, model statistics can be included in the report by either appending them to the report table with [`add_glance_table()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html), or adding them as a table source note with [`add_glance_source_note()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html).

```{r}
tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

### Checking for multicolinearity

When performing OLS regression, we can use the **olsrr** package, which provides a collection of useful methods for building better multiple linear regression models, including:

-   Comprehensive regression output

-   Residual diagnostics

-   Measures of influence

-   Heteroskedasticity tests

-   Collinearity diagnostics

-   Model fit assessment

-   Variable contribution assessment

-   Variable selection procedures

Additionally, the [`ols_vif_tol()`](https://olsrr.rsquaredacademy.com/reference/ols_coll_diag.html) function from the **olsrr** package is used to check for strong signs of multicollinearity.

```{r}
ols_vif_tol(condo.mlr1)
```

> Since the VIF values of the independent variables are all less than 10, we can safely conclude that there are no signs of multicollinearity among the independent variables.

### Test for Non-Linearity

In multiple linear regression, it is important to test the assumption of linearity and additivity in the relationship between the dependent and independent variables. To test the linearity assumption, we use the [`ols_plot_resid_fit()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html) function from the **olsrr** package.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

> The figure above shows that most of the data points are scattered around the zero line, allowing us to conclude that the relationships between the dependent variable and the independent variables are linear.

### Test for Normality Assumption

We can use [`ols_plot_resid_hist()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_hist.html) of *olsrr* package to perform normality assumption test.

```{r}
ols_plot_resid_hist(condo.mlr1)
```

> The figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.

For formal statistical test methods, the [`ols_test_normality()`](https://olsrr.rsquaredacademy.com/reference/ols_test_normality.html) of **olsrr** package can be used as well.

```{r}
ols_test_normality(condo.mlr1)
```

> The summary table above shows that the p-values for all four tests are much smaller than the alpha value of 0.05. Therefore, we reject the null hypothesis and conclude that there is statistical evidence that the residuals are not normally distributed.

### Testing for Spatial Autocorrelation

The hedonic model we are building uses geographically referenced attributes, so it is important to visualize the residuals of the hedonic pricing model.

To perform a spatial autocorrelation test, we first need to convert the **condo_resale.sf** object from an **sf** data frame into a **SpatialPointsDataFrame**.

#### Export the residual of hedonic pricing model

**Export the residual** of the hedonic pricing model and save it as a data frame.

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)
```

#### Join with condo_resale.sf object

Join the newly created data frame with *condo_resale.sf* object.

```{r}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

#### Convert to SpatialPointsDataFrame

Convert `condo_resale.res.sf` simple feature object into a SpatialPointsDataFrame because **spdep** package can only process sp conformed spatial data objects

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

#### Plot interactive point symbol map

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

> The figure above reveal that there is sign of spatial autocorrelation.
>
> To proof that our observation is indeed true, the Moran’s I test will be performed

#### Moran's I test

To perform Moran's I test, we will compute the distance-based weight matrix by using [`dnearneigh()`](https://r-spatial.github.io/spdep/reference/dnearneigh.html) function of **spdep**.

```{r}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

Next, [`nb2listw()`](https://r-spatial.github.io/spdep/reference/nb2listw.html) of **spdep** packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.

```{r}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

Next, [`lm.morantest()`](https://r-spatial.github.io/spdep/reference/lm.morantest.html) of **spdep** package will be used to perform Moran’s I test for residual spatial autocorrelation

```{r}
lm.morantest(condo.mlr1, nb_lw)
```

> The Global Moran’s I test for residual spatial autocorrelation indicates that its p-value is less than 0.00000000000000022, which is below the alpha level of 0.05. Hence, we reject the null hypothesis that the residuals are randomly distributed.
>
> Since the observed Global Moran’s I value is 0.1424418, which is greater than zero, we can infer that the residuals exhibit a clustered spatial distribution.

# Building Hedonic Pricing Models using GW

In this section, we will learn how to modelling hedonic pricing using both the **fixed** and **adaptive** bandwidth schemes.

## Building Fixed Bandwidth GWR model

### Compute fixed bandwidth

In the code chunk below, the `bw.gwr()` function from the **GWmodel** package is used to determine the optimal fixed bandwidth for the model. Notice that the argument `adaptive` is set to `FALSE`, indicating that we are interested in computing a fixed bandwidth.

There are two possible approaches that can be used to determine the stopping rule: the cross-validation (CV) approach and the corrected Akaike Information Criterion (AICc) approach. We define the stopping rule using an agreement between these approaches.

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                     FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

> The results indicate that the recommended bandwidth is **971.34 meters**.
>
> The projection coordinate system used is **SVY21**, which operates in meters. This explains why the results are presented in meters.

### Construct the fixed bandwidth gwr model

We will calibrate the **GWR (Geographically Weighted Regression)** model using a fixed bandwidth and a Gaussian kernel. The output is saved as a list of class `gwrm`.

```{r}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                         FAMILY_FRIENDLY + FREEHOLD, 
                       data=condo_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
gwr.fixed
```

> The report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.14.

## Building Adaptive Bandwidth GWR Model

In this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.

### Compute adaptive bandwidth

-   Similar to the earlier section, we will first use `bw.gwr()` to determine the recommended data point to use.

-   Note: adaptive argument set to TRUE.

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + 
                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + 
                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + 
                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

> The result shows that the **30** is the recommended data points to be used.

### Construct the adaptive bandwidth gwr model

We will now calibrate the **GWR-based hedonic pricing model** using an **adaptive bandwidth** and a **Gaussian kernel**.

```{r}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
gwr.adaptive
```

> The report shows that the **AICc** for the **adaptive bandwidth GWR** is **41982.22**, which is smaller than the AICc of the **fixed bandwidth GWR** at **42263.61**.

## Visualising GWR Output

In addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:

-   **Condition Number**: evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers **larger than 30**, may be **unreliable**

-   **Local R2**: these values **range between 0.0 and 1.0** and indicate **how well the local regression model fits observed y values**.

    -   **Very low values** indicate the local model is performing **poorly**.

    -   Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.

-   **Predicted**: estimated (or fitted) y values computed by GWR.

-   **Residuals**: to obtain the residual values, the fitted y values are subtracted from the observed y values.

    -   Standardized residuals have a mean of zero and a standard deviation of 1.

    -   A cold-to-hot rendered map of standardized residuals can be produce by using these values.

-   **Coefficient Standard Error**: these values measure the reliability of each coefficient estimate.

    -   **Confidence** in those estimates are **higher** when **standard errors are small** in relation to the actual coefficient values.

    -   **Large standard errors** may indicate **problems with local collinearity**.

They are all stored in a **SpatialPointsDataFrame** or **SpatialPolygonsDataFrame** object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called **SDF** of the output list.

### Converting SDF into sf data.frame

To visualise the fields in **SDF**, we need to first convert it into **sf** data.frame.

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)
```

```{r}
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
condo_resale.sf.adaptive.svy21  
```

```{r}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))
```

```{r}
glimpse(condo_resale.sf.adaptive)
```

```{r}
summary(gwr.adaptive$SDF$yhat)
```

## Visualising local R2

Now we will create an interactive point symbol map.

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

## Visualising coefficient estimates

```{r}
tmap_mode("view")
AREA_SQM_SE <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

AREA_SQM_TV <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=2,
             sync = TRUE)
tmap_mode("plot")
```

# Reference

Kam, T. S. Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method. *R for Geospatial Data Science and Analytics.* <https://r4gdsa.netlify.app/chap13.html>
